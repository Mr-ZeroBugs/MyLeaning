import torch.nn.functional as f

f.softmax() #ลดขนาดข้อมูล (หากใช้ cross ไม่ต้องใช้ softmax)
f.leaky_relu() #ใช้เเก้ปัญหาเมื่อใช้ relu เเล้วค่าเป็นลบตลอดเวลา, leaky จะทำให้พวกค่าลบเป็นค่าที่เล็กๆมาก เเต่ไม่ใช่0 เเทน
f.relu() #จะทำให้data ที่มีผลเป็น - เเปลงให้เป็น 0 ไปเลย
f.tanh()  #เหมือน sigmoid เเต่ช่วงค่าเป็น -1 ถึง 1
f.sigmoid() #เเบ่งคาสเป็น 0 กับ 1, ใกล้เคียงอันไหนก็อันนั้นเเหละ ตามหลักการปัดเศษทั่วไป
