{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFcEMULXGtoOVz6zgGw1bz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iLikeKatz/MyLeaning_ML/blob/main/PytorchFullCourse/Models/Multi_Cassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "XWQLv97yABPP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_samples = 1000\n",
        "n_features  = 2\n",
        "n_classes = 4\n",
        "x, y = make_blobs(n_samples =n_samples, n_features=2, centers=n_classes, random_state=42, cluster_std=1.5)\n",
        "x = torch.from_numpy(x).to(device).type(torch.float)\n",
        "y = torch.from_numpy(y).to(device).type(torch.long)"
      ],
      "metadata": {
        "id": "dGcNNKwr-grC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "id": "obljIKno_i7o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baee0142-2ab3-4e15-b382-50bc45c37a13"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1000, 2]), torch.Size([1000]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi(nn.Module):\n",
        "  def __init__(self, in_features, out_features, hidden=8):\n",
        "    super().__init__()\n",
        "    self.stack_layers = nn.Sequential(\n",
        "        nn.Linear(in_features, hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden, hidden),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden, out_features)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.stack_layers(x)\n",
        "    return x\n",
        "\n",
        "model = Multi(in_features=n_features, out_features=n_classes, hidden=12).to(device)\n",
        "model.state_dict()"
      ],
      "metadata": {
        "id": "01uVGz_l_zxg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "epochs = 2000\n"
      ],
      "metadata": {
        "id": "SNlP-zmwCd8o"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  model.train()\n",
        "  y_logits= model(x_train) #the output will be the values of each columns\n",
        "  y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) #so we need to use softmax to convert each values of columns to probabilities and then we will use .argmax in dim1 to show the most probability values in each rows\n",
        "\n",
        "  loss = loss_fn(y_logits, y_train)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    y_logits_ = model(x_test)\n",
        "    y_pred_  = torch.softmax(y_logits_, dim=1).argmax(dim=1)\n",
        "    loss_test = loss_fn(y_logits_, y_pred_)\n",
        "    if epoch % 100 == 0:\n",
        "      print(f\"epoch : {epoch}, loss : {loss}, loss_test : {loss_test}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DORx1ZstCuHA",
        "outputId": "530b8400-905a-493e-b2be-7b404c7bd28e"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0, loss : 1.1694415807724, loss_test : 0.965263843536377\n",
            "epoch : 100, loss : 0.021410437300801277, loss_test : 0.007150260265916586\n",
            "epoch : 200, loss : 0.018543459475040436, loss_test : 0.0072233593091368675\n",
            "epoch : 300, loss : 0.017083974555134773, loss_test : 0.005195074714720249\n",
            "epoch : 400, loss : 0.016519244760274887, loss_test : 0.004064951557666063\n",
            "epoch : 500, loss : 0.016083573922514915, loss_test : 0.003563585923984647\n",
            "epoch : 600, loss : 0.015493012964725494, loss_test : 0.0032390370033681393\n",
            "epoch : 700, loss : 0.014826472848653793, loss_test : 0.0028812831733375788\n",
            "epoch : 800, loss : 0.01416113879531622, loss_test : 0.0026906474959105253\n",
            "epoch : 900, loss : 0.013566549867391586, loss_test : 0.002675306284800172\n",
            "epoch : 1000, loss : 0.013277177698910236, loss_test : 0.0021135404240339994\n",
            "epoch : 1100, loss : 0.012862886302173138, loss_test : 0.001990766730159521\n",
            "epoch : 1200, loss : 0.013253421522676945, loss_test : 0.0014263041084632277\n",
            "epoch : 1300, loss : 0.012425441294908524, loss_test : 0.0018803556449711323\n",
            "epoch : 1400, loss : 0.012732753530144691, loss_test : 0.0014558773254975677\n",
            "epoch : 1500, loss : 0.011960548348724842, loss_test : 0.001537587377242744\n",
            "epoch : 1600, loss : 0.012118089012801647, loss_test : 0.002120337216183543\n",
            "epoch : 1700, loss : 0.011841624043881893, loss_test : 0.00228427373804152\n",
            "epoch : 1800, loss : 0.011588800698518753, loss_test : 0.0015921314479783177\n",
            "epoch : 1900, loss : 0.011660685762763023, loss_test : 0.0016276531387120485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score as acc\n",
        "model.eval()\n",
        "with torch.inference_mode():\n",
        "    y_logits__ = model(x_test)\n",
        "    y_pred__  = torch.softmax(y_logits__, dim=1).argmax(dim=1)\n",
        "    loss_test = loss_fn(y_logits__, y_pred__)\n",
        "acc(y_pred__, y_test) * 100"
      ],
      "metadata": {
        "id": "sNc0Pa-aE7FQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb4f10f-6edb-468d-94a3-80fa91dc034b"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    }
  ]
}